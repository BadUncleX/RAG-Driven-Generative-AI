{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqpsYn49QWSR"
      },
      "source": [
        "#Introducing Na誰ve, Advanced, and Modular RAG\n",
        "\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "This notebook introduces Na誰ve, Advanced, and Modular RAG through basic educational examples.\n",
        "\n",
        "It explores keyword matching, vector search, and index-based retrieval methods. Using OpenAI's GPT models, it generates responses based on input queries and retrieved documents.\n",
        "\n",
        "The modular RAG system offers flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "- Introduction of Na誰ve, Advanced, and Modular RAG\n",
        "- Environment setup for OpenAI API integration\n",
        "- Generator function using GPT models\n",
        "- Formatted response printing\n",
        "- **Data** setup with a list of documents (db_records)\n",
        "- **Query** user request\n",
        "- **1.Na誰ve RAG**:\n",
        "  - Keyword search and matching function\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT\n",
        "- **2.Advanced RAG**:\n",
        "  - Vector search:\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "  - Index-based retrieval:\n",
        "    - Setup of TF-IDF vectorizer and matrix\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "- **3.Modular RAG Retriever**:\n",
        "  - RetrievalComponent class with methods for keyword, vector, and indexed search\n",
        "  - Usage example with different retrieval methods\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o01-IM8bTc5f"
      },
      "source": [
        "# The Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VCfbN0YwHbE",
        "outputId": "d2051b87-ad47-43de-8541-498d5981d159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.19.0 in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.19.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.19.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (2.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myXJn33zbqTR",
        "outputId": "94f6bf20-fc40-4ef1-cd4a-ce6997259504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#API Key\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Oefvqp21Ba07"
      },
      "outputs": [],
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuZ7jr4Rs36U"
      },
      "source": [
        "# The Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "qwCNTW9fs36U"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\"\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVKe9VF0HIHJ"
      },
      "source": [
        "### Formatted response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "oG8I2Kb2HFhL"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv1ExPiZdJRL"
      },
      "source": [
        " # The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "45CFxG4Fgcju"
      },
      "outputs": [],
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "FIQ7NK92g7EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c732222-2733-4d1b-a24e-6389a05961e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form. A RAG vector store is a database or\n",
            "dataset that contains vectorized data points.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL7cHuuLhQ5w"
      },
      "source": [
        "## The Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "qARk6gtohSXW"
      },
      "outputs": [],
      "source": [
        "query = \"define a rag store\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4lLAvV6NIn_"
      },
      "source": [
        "### Generation without augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "iO-k1Tq4MqmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a873ce1-e47a-4510-bda8-df79fd989460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! The content you've provided appears to be a sequence of characters\n",
            "that, when combined, form the phrase \"define a rag store.\" Let's break it down\n",
            "step by step:  1. **Characters**: The sequence consists of individual\n",
            "characters:    - d    - e    - f    - i    - n    - e    - (space)    - a    -\n",
            "(space)    - r    - a    - g    - (space)    - s    - t    - o    - r    - e  2.\n",
            "**Combining Characters**: When these characters are combined, they form the\n",
            "phrase \"define a rag store.\"  3. **Understanding the Phrase**:    - **Define**:\n",
            "This is a verb that means to explain the meaning of a word or concept.    -\n",
            "**a**: This is an indefinite article used before words that begin with a\n",
            "consonant sound.    - **rag**: This is a noun that typically refers to a piece\n",
            "of old, often torn, cloth.    - **store**: This is a noun that refers to a place\n",
            "where goods are sold.  4. **Contextual Meaning**:    - **\"Define a rag store\"**:\n",
            "This phrase is asking for an explanation or definition of what a \"rag store\" is.\n",
            "5. **Possible Definition**:    - A \"rag store\" could be a shop or retail\n",
            "establishment that specializes in selling rags, which are pieces of old cloth\n",
            "often used for cleaning or other purposes. It might also sell other related\n",
            "items such as cleaning supplies or recycled textiles.  Would you like more\n",
            "information or a different type of elaboration on this content?\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFqh9rr81SUn"
      },
      "source": [
        "# 1.Na誰ve Retrieval Augmented Generation(RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8vteKmS_qO"
      },
      "source": [
        "## Keyword search and matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "WY1JU0Ush_l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972a763b-814a-4367-fc0a-96515c0bbecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 3\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zKQhiO0Fcr"
      },
      "source": [
        "## Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "r_7ymSxG0Fcs"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "7NTfxzum0PT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bada8cf-c459-4d0e-829c-4cda9582a8ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeA RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ui8wH4k3_g4"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Jh8BsnUy0Fcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957e6fd5-4a9e-44a2-f31a-d640418e9a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "Vector Store  A **vector store** is a specialized type of database or dataset\n",
            "designed to store and manage vectorized data points. Let's delve into the key\n",
            "components of this definition:  1. **Vectorized Data Points**:    - **Vectors**:\n",
            "In the context of data science and machine learning, vectors are arrays of\n",
            "numbers that represent data points in a multi-dimensional space. Each dimension\n",
            "corresponds to a feature or attribute of the data.    - **Vectorization**: This\n",
            "is the process of converting data into a numerical format that can be used for\n",
            "various computational tasks. For example, text can be vectorized using\n",
            "techniques like word embeddings (e.g., Word2Vec, GloVe) or more advanced methods\n",
            "like BERT embeddings.  2. **Database or Dataset**:    - **Database**: A\n",
            "structured collection of data that is stored and accessed electronically. In the\n",
            "case of a vector store, the database is optimized for storing and retrieving\n",
            "vectorized data points efficiently.    - **Dataset**: A collection of related\n",
            "data points that are typically used for analysis or training machine learning\n",
            "models. When we refer to a vector store as a dataset, we imply that it is a\n",
            "collection of vectorized data points ready for use in various applications.  ###\n",
            "Applications of a Vector Store  1. **Machine Learning**:    - **Training\n",
            "Models**: Vector stores are often used to store training data for machine\n",
            "learning models. The vectorized format allows for efficient computation and\n",
            "manipulation of data during the training process.    - **Feature Storage**: In\n",
            "many machine learning pipelines, features are extracted and stored as vectors.\n",
            "These features can then be used for training, validation, and testing of models.\n",
            "2. **Information Retrieval**:    - **Search Engines**: Vector stores are used in\n",
            "search engines to store document embeddings. When a query is made, the search\n",
            "engine can quickly retrieve and rank documents based on their vector similarity\n",
            "to the query.    - **Recommendation Systems**: Vector stores can be used to\n",
            "store user and item embeddings. Recommendations can be made by finding items\n",
            "with vectors similar to the user's vector.  3. **Natural Language Processing\n",
            "(NLP)**:    - **Text Embeddings**: NLP applications often use vector stores to\n",
            "manage text embeddings. These embeddings capture the semantic meaning of text\n",
            "and can be used for tasks like sentiment analysis, text classification, and\n",
            "more.    - **Similarity Search**: Vector stores enable efficient similarity\n",
            "search, which is crucial for tasks like finding similar documents, clustering,\n",
            "and anomaly detection.  ### Advantages of Using a Vector Store  1.\n",
            "**Efficiency**: Vector stores are optimized for the storage and retrieval of\n",
            "high-dimensional data, making them highly efficient for tasks that involve large\n",
            "datasets. 2. **Scalability**: They can handle large volumes of data and scale\n",
            "horizontally, making them suitable for big data applications. 3.\n",
            "**Flexibility**: Vector stores can be used with various types of data, including\n",
            "text, images, and audio, as long as the data can be vectorized.  ### Conclusion\n",
            "A vector store is a powerful tool in the realm of data science and machine\n",
            "learning. By storing data in a vectorized format, it enables efficient\n",
            "computation, retrieval, and analysis of complex data points. Whether used in\n",
            "machine learning, information retrieval, or NLP, vector stores play a crucial\n",
            "role in modern data-driven applications.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJH2__0iTUr1"
      },
      "source": [
        "# 2.Advanced Retrieval Augmented Generation(RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awyjcn35jFiy"
      },
      "source": [
        "## 2.1.Vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "FCBbY4qLc8qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae63ea8-566a-4d70-e18c-8b14417c902d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.215\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record\n",
        "\n",
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fZC6Oe2G9E"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "4dcnK7OGx5e6"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "T3uk-91x049J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660ea03f-2954-4aad-9d5f-26a81dde67b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFDF6hbi2LF9"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "YJC-mA5ftxFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1231889c-14cd-40a6-bf63-3fa7d807c2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ---\n",
            "**Define a RAG store:**  A **RAG (Retrieval-Augmented Generation) vector store**\n",
            "is a specialized type of database or dataset that contains vectorized data\n",
            "points.  ### Key Concepts:  1. **RAG (Retrieval-Augmented Generation):**    -\n",
            "**Retrieval-Augmented Generation** is a technique in natural language processing\n",
            "(NLP) that combines the strengths of retrieval-based and generation-based\n",
            "models. It retrieves relevant information from a large dataset and uses it to\n",
            "generate more accurate and contextually appropriate responses or outputs.  2.\n",
            "**Vector Store:**    - A **vector store** is a storage system designed to hold\n",
            "and manage vectorized representations of data. In the context of NLP, these\n",
            "vectors are often high-dimensional representations of text, images, or other\n",
            "data types, created using techniques like word embeddings, sentence embeddings,\n",
            "or other forms of feature extraction.  ### Detailed Explanation:  - **Database\n",
            "or Dataset:**   - A RAG vector store can be thought of as a database or dataset\n",
            "specifically designed to store vectors. These vectors are numerical\n",
            "representations of data points, which can be anything from words and sentences\n",
            "to images and other forms of data.  - **Vectorized Data Points:**   -\n",
            "**Vectorization** is the process of converting data into a numerical format that\n",
            "can be easily processed by machine learning models. For text data, this often\n",
            "involves techniques like Word2Vec, GloVe, or BERT, which transform words or\n",
            "sentences into high-dimensional vectors.   - These vectors capture the semantic\n",
            "meaning of the data, allowing for more sophisticated analysis and retrieval.\n",
            "### Applications:  - **Information Retrieval:**   - In a RAG system, the vector\n",
            "store is used to quickly retrieve relevant information based on the input query.\n",
            "The retrieved vectors can then be used to generate more accurate and\n",
            "contextually relevant responses.    - **Natural Language Processing:**   - RAG\n",
            "vector stores are particularly useful in NLP tasks such as question answering,\n",
            "summarization, and conversational AI, where the ability to retrieve and generate\n",
            "information dynamically is crucial.  ### Example:  Imagine you have a large\n",
            "collection of documents. Each document is vectorized and stored in a RAG vector\n",
            "store. When a user asks a question, the system retrieves the most relevant\n",
            "document vectors and uses them to generate a coherent and accurate answer.  ---\n",
            "In summary, a RAG vector store is a powerful tool in the field of NLP, combining\n",
            "the capabilities of retrieval and generation to enhance the performance of\n",
            "various applications.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7djpPBpm0M2"
      },
      "source": [
        "## 2.2.Index-based search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "wRarT_fym2XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac0e03ad-0fb6-4007-e58c-b834c7a605ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.407\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "SbokQ2eacHjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6e9a91-a092-450b-b062-c0b14a96069d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
            "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "\n",
            "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
            "\n",
            "       where     which    while     wide      with    within   without  \n",
            "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
            "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
            "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[28 rows x 297 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(tfidf_df)\n",
        "\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dABZ12Bkugtt"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "1w4wppuA4eNn"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "MNozI65K4e7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d038f576-a856-4388-b0a6-944a94288805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU998zkD4hpD"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "uy9X-l_Iugtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284b1ad8-72c1-4916-bf74-e6d7de6ee1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the provided content:  ---\n",
            "**Define a RAG store:**  A **RAG (Retrieval-Augmented Generation)** store is a\n",
            "specialized type of data storage system that is designed to support the\n",
            "retrieval and generation of information. It typically combines elements of both\n",
            "traditional databases and advanced machine learning models to enhance the\n",
            "process of generating responses or retrieving relevant information.  **ARA\n",
            "(Approximate Nearest Neighbor) vector store:**  An **ARA (Approximate Nearest\n",
            "Neighbor)** vector store is a type of database or dataset that is optimized for\n",
            "storing and retrieving high-dimensional vectorized data points efficiently.\n",
            "These vectorized data points are often the result of transforming raw data (such\n",
            "as text, images, or other forms of unstructured data) into numerical vectors\n",
            "using various machine learning techniques.  **Vector store:**  A **vector\n",
            "store** is a database or dataset that contains vectorized data points. These\n",
            "vectors are numerical representations of data that capture the essential\n",
            "features and relationships within the data. Vector stores are commonly used in\n",
            "machine learning and artificial intelligence applications, particularly in tasks\n",
            "involving similarity search, clustering, and classification.  **Key Points:**\n",
            "1. **Vectorized Data Points:**    - Vectorized data points are numerical\n",
            "representations of raw data.    - These vectors capture the essential features\n",
            "and relationships within the data.    - Commonly used in machine learning and AI\n",
            "applications.  2. **Storage and Retrieval:**    - Vector stores are optimized\n",
            "for efficient storage and retrieval of high-dimensional vectors.    - They\n",
            "support tasks such as similarity search, clustering, and classification.  3.\n",
            "**Applications:**    - Used in various applications, including natural language\n",
            "processing, image recognition, and recommendation systems.    - Enhance the\n",
            "process of generating responses or retrieving relevant information.  4.\n",
            "**Approximate Nearest Neighbor (ANN):**    - ANN techniques are often used in\n",
            "vector stores to quickly find the nearest vectors to a given query vector.    -\n",
            "These techniques balance accuracy and efficiency, making them suitable for\n",
            "large-scale datasets.  ---  In summary, a RAG store, particularly an ARA vector\n",
            "store, is a specialized database designed to handle vectorized data points\n",
            "efficiently. It plays a crucial role in modern machine learning and AI\n",
            "applications by enabling fast and accurate retrieval of relevant information\n",
            "based on vector representations of data.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWEvzcDHTX6i"
      },
      "source": [
        "# 3.Modular RAG Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "18wmqwJd4o62"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHm4saJ8cGk"
      },
      "source": [
        "### Modular RAG Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "_kvhIOdY8amp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613afe21-5aca-4597-9d41-7c9ce6f0e6ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TaQa7Dc7JwT"
      },
      "source": [
        "### Augmented Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "X-hKjhIU7Jwg"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "ZhSO-fyZ7Jwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab27c61c-dc67-4aaf-af1b-8ba4f707e9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeA RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkyYx_MC7Jwg"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "-V3srRHW7Jwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d674ebf3-f485-4b14-bee1-d6f0da24c932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "Vector Store  A **vector store** is a specialized type of database or dataset\n",
            "designed to store and manage vectorized data points. Vectorized data points are\n",
            "numerical representations of data, often used in machine learning and natural\n",
            "language processing (NLP) tasks. These vectors can represent various types of\n",
            "data, such as text, images, or other forms of information, in a way that makes\n",
            "them suitable for computational analysis.  #### Key Components of a Vector\n",
            "Store:  1. **Vectorized Data Points**:    - These are the core elements stored\n",
            "in a vector store. Each data point is converted into a vector, which is a list\n",
            "or array of numbers. For example, in NLP, words or sentences can be transformed\n",
            "into vectors using techniques like Word2Vec, GloVe, or BERT embeddings.  2.\n",
            "**Storage Mechanism**:    - The vector store must efficiently store these high-\n",
            "dimensional vectors. This often involves specialized data structures and\n",
            "indexing methods to allow for quick retrieval and similarity searches.  3.\n",
            "**Similarity Search**:    - One of the primary functions of a vector store is to\n",
            "enable similarity searches. This means finding vectors that are close to a given\n",
            "vector in terms of some distance metric (e.g., Euclidean distance, cosine\n",
            "similarity). This is crucial for tasks like nearest neighbor search, clustering,\n",
            "and recommendation systems.  4. **Scalability**:    - A good vector store should\n",
            "be able to handle large volumes of data and scale as the dataset grows. This\n",
            "involves optimizing storage and retrieval processes to maintain performance.  5.\n",
            "**Integration with Machine Learning Pipelines**:    - Vector stores are often\n",
            "integrated into broader machine learning and data processing pipelines. They\n",
            "serve as repositories for intermediate representations of data that can be used\n",
            "for training models, making predictions, or performing analyses.  ####\n",
            "Applications of Vector Stores:  1. **Natural Language Processing (NLP)**:    -\n",
            "In NLP, vector stores are used to store word embeddings, sentence embeddings, or\n",
            "document embeddings. These embeddings are used for tasks like text\n",
            "classification, sentiment analysis, and information retrieval.  2. **Image\n",
            "Processing**:    - In image processing, vector stores can store feature vectors\n",
            "extracted from images. These vectors can be used for image recognition,\n",
            "clustering similar images, or finding duplicate images.  3. **Recommendation\n",
            "Systems**:    - Vector stores are used to store user and item embeddings in\n",
            "recommendation systems. These embeddings help in finding similar users or items\n",
            "to make personalized recommendations.  4. **Anomaly Detection**:    - In anomaly\n",
            "detection, vector stores can store normal behavior patterns as vectors. New data\n",
            "points can be compared against these stored vectors to detect anomalies.  In\n",
            "summary, a vector store is a crucial component in modern data processing and\n",
            "machine learning systems, enabling efficient storage, retrieval, and analysis of\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNY6GZrQlF+CaxnJmP56n6c"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
