{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4GkKBCs7amgD4AvQrh3wI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introducing Naïve, Advanced, and Modular RAG\n",
        "\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "This notebook introduces Naïve, Advanced, and Modular RAG through basic educational examples.\n",
        "\n",
        "It explores keyword matching, vector search, and index-based retrieval methods. Using OpenAI's GPT models, it generates responses based on input queries and retrieved documents.\n",
        "\n",
        "The modular RAG system offers flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "- Introduction of Naïve, Advanced, and Modular RAG\n",
        "- Environment setup for OpenAI API integration\n",
        "- Generator function using GPT models\n",
        "- Formatted response printing\n",
        "- **Data** setup with a list of documents (db_records)\n",
        "- **Query** user request\n",
        "- **1.Naïve RAG**:\n",
        "  - Keyword search and matching function\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT\n",
        "- **2.Advanced RAG**:\n",
        "  - Vector search:\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "  - Index-based retrieval:\n",
        "    - Setup of TF-IDF vectorizer and matrix\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "- **3.Modular RAG Retriever**:\n",
        "  - RetrievalComponent class with methods for keyword, vector, and indexed search\n",
        "  - Usage example with different retrieval methods\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT"
      ],
      "metadata": {
        "id": "sqpsYn49QWSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Environment"
      ],
      "metadata": {
        "id": "o01-IM8bTc5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.19.0"
      ],
      "metadata": {
        "id": "8VCfbN0YwHbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8350072-cead-4916-a65d-bd6f2720b518"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.19.0 in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.19.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.19.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#API Key\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef5ad64-a27f-49bb-a6bd-c3454441d445",
        "id": "myXJn33zbqTR"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "Oefvqp21Ba07"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Generator\n"
      ],
      "metadata": {
        "id": "WuZ7jr4Rs36U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\" # or select gpt-3.5-turbo\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "def call_gpt4_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ],
      "metadata": {
        "id": "qwCNTW9fs36U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formatted response"
      ],
      "metadata": {
        "id": "bVKe9VF0HIHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ],
      "metadata": {
        "id": "oG8I2Kb2HFhL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # The Data"
      ],
      "metadata": {
        "id": "Qv1ExPiZdJRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\"\n",
        "]"
      ],
      "metadata": {
        "id": "45CFxG4Fgcju"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "FIQ7NK92g7EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865c433b-ba73-49e8-c210-bad47689ab29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Query"
      ],
      "metadata": {
        "id": "aL7cHuuLhQ5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"define a rag store\""
      ],
      "metadata": {
        "id": "qARk6gtohSXW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation without augmentation"
      ],
      "metadata": {
        "id": "U4lLAvV6NIn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(query)\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "id": "iO-k1Tq4MqmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47af4c3-9e22-43b6-e043-00bddeb1624b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! The content you've provided appears to be a sequence of characters\n",
            "that, when combined, form the phrase \"define a rag store.\" Let's break it down\n",
            "step by step:  1. **Characters Provided:**    - d    - e    - f    - i    - n\n",
            "- e    - (space)    - a    - (space)    - r    - a    - g    - (space)    - s\n",
            "- t    - o    - r    - e  2. **Combining Characters:**    When you combine these\n",
            "characters in the order given, you get the phrase: \"define a rag store.\"  3.\n",
            "**Understanding the Phrase:**    - **Define:** This is a verb that means to\n",
            "explain the meaning of a word or concept.    - **a:** This is an indefinite\n",
            "article used before words that begin with a consonant sound.    - **rag:** This\n",
            "is a noun that typically refers to a piece of old, often torn, cloth.    -\n",
            "**store:** This is a noun that refers to a place where goods are sold.  4.\n",
            "**Interpreting the Phrase:**    The phrase \"define a rag store\" is essentially\n",
            "asking for an explanation or definition of what a \"rag store\" is.  5. **Possible\n",
            "Definition of a Rag Store:**    A \"rag store\" could be interpreted as a shop or\n",
            "retail establishment that specializes in selling rags or old pieces of cloth.\n",
            "These stores might sell rags for various purposes, such as cleaning, crafting,\n",
            "or industrial use.  6. **Contextual Use:**    - In historical contexts, a rag\n",
            "store might have been a common place where people could buy inexpensive cloth\n",
            "materials.    - In modern times, it could refer to a niche store that caters to\n",
            "specific needs for rags, such as for artists, mechanics, or cleaning services.\n",
            "By breaking down the sequence of characters and understanding the phrase they\n",
            "form, we can provide a detailed explanation of what a \"rag store\" might be.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Naïve Retrieval Augmented Generation(RAG)"
      ],
      "metadata": {
        "id": "JFqh9rr81SUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword search and matching"
      ],
      "metadata": {
        "id": "Wu8vteKmS_qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "WY1JU0Ush_l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704c8bb1-2e27-4ab8-fa54-5e748cdf7c73"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 1\n",
            "Response:\n",
            "---------------\n",
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmented input"
      ],
      "metadata": {
        "id": "r2zKQhiO0Fcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+best_matching_record"
      ],
      "metadata": {
        "id": "r_7ymSxG0Fcs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "7NTfxzum0PT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01212125-5827-41ad-a0ee-866a0757f205"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeRetrieval Augmented Generation (RAG) represents a\n",
            "sophisticated hybrid approach in the field of artificial intelligence,\n",
            "particularly within the realm of natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "8Ui8wH4k3_g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "id": "Jh8BsnUy0Fcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19558cec-d497-4ce8-cf9f-4686c2661179"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ---\n",
            "**Define a Retrieval-Augmented Generation (RAG)**  **Retrieval-Augmented\n",
            "Generation (RAG)** represents a sophisticated hybrid approach in the field of\n",
            "artificial intelligence, particularly within the realm of natural language\n",
            "processing (NLP).  ---  ### Detailed Explanation:  **1. Retrieval-Augmented\n",
            "Generation (RAG):** RAG is a cutting-edge technique that combines two powerful\n",
            "methods in NLP: retrieval-based methods and generation-based methods.  -\n",
            "**Retrieval-Based Methods:** These methods involve searching a large corpus of\n",
            "text to find relevant information or documents that can help answer a query.\n",
            "This is similar to how search engines work, where they retrieve the most\n",
            "relevant documents based on the user's query.  - **Generation-Based Methods:**\n",
            "These methods involve generating text based on a given input. This is akin to\n",
            "how language models like GPT-3 work, where they generate coherent and\n",
            "contextually relevant text based on the input they receive.  **2. Hybrid\n",
            "Approach:** RAG combines the strengths of both retrieval-based and generation-\n",
            "based methods. By doing so, it can provide more accurate and contextually\n",
            "relevant responses. Here's how it works:  - **Retrieval Step:** When a query is\n",
            "received, the system first retrieves relevant documents or pieces of information\n",
            "from a large corpus. This ensures that the system has access to accurate and\n",
            "relevant information.  - **Generation Step:** Using the retrieved information,\n",
            "the system then generates a response. This allows the system to produce more\n",
            "coherent and contextually appropriate answers, as it is grounded in the\n",
            "retrieved information.  **3. Field of Artificial Intelligence:** RAG is a part\n",
            "of the broader field of artificial intelligence, which involves creating systems\n",
            "that can perform tasks that typically require human intelligence. This includes\n",
            "understanding and generating human language, recognizing patterns, making\n",
            "decisions, and more.  **4. Natural Language Processing (NLP):** NLP is a\n",
            "subfield of artificial intelligence that focuses on the interaction between\n",
            "computers and human language. It involves enabling computers to understand,\n",
            "interpret, and generate human language in a way that is both meaningful and\n",
            "useful. RAG is a significant advancement within NLP, as it enhances the ability\n",
            "of systems to provide accurate and contextually relevant responses.  ---  ###\n",
            "Summary: Retrieval-Augmented Generation (RAG) is an advanced technique in\n",
            "artificial intelligence, specifically within natural language processing. It\n",
            "combines retrieval-based methods, which search for relevant information, with\n",
            "generation-based methods, which generate text. This hybrid approach allows for\n",
            "more accurate and contextually appropriate responses, making it a powerful tool\n",
            "in the realm of NLP.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Advanced Retrieval Augmented Generation(RAG)"
      ],
      "metadata": {
        "id": "zJH2__0iTUr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.Vector search"
      ],
      "metadata": {
        "id": "awyjcn35jFiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record\n",
        "\n",
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "FCBbY4qLc8qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c56fb3-e7fe-477e-ac66-1ce6a9aaafae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.087\n",
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented input"
      ],
      "metadata": {
        "id": "51fZC6Oe2G9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ],
      "metadata": {
        "id": "4dcnK7OGx5e6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "T3uk-91x049J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b57d2b9-9881-4c47-a845-277ce476f20f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store While Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "wFDF6hbi2LF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "id": "YJC-mA5ftxFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada16fe2-2e9e-4b0d-99f4-ef1d05ab4e10"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "RAG (Retrieval-Augmented Generation)  **Retrieval-Augmented Generation (RAG)**\n",
            "is a sophisticated approach in the field of Natural Language Processing (NLP)\n",
            "that combines the strengths of retrieval-based methods and generative models.\n",
            "Here's a detailed explanation:  1. **Retrieval Component**:    - **Function**:\n",
            "The retrieval component is responsible for fetching relevant information from a\n",
            "large corpus or database. This is akin to how search engines work, where given a\n",
            "query, the system retrieves the most pertinent documents or pieces of\n",
            "information.    - **Benefit**: This ensures that the generative model has access\n",
            "to accurate and up-to-date information, which is particularly useful for tasks\n",
            "requiring factual correctness.  2. **Generation Component**:    - **Function**:\n",
            "The generative model, typically a neural network like GPT-3, takes the retrieved\n",
            "information and generates coherent and contextually appropriate responses or\n",
            "text.    - **Benefit**: This allows for more flexible and creative responses, as\n",
            "the generative model can synthesize information in novel ways.  ### Benefits of\n",
            "RAG  1. **Enhanced Accuracy**:    - By leveraging a retrieval mechanism, the\n",
            "generative model can produce more accurate and factually correct responses, as\n",
            "it has access to a wealth of information.  2. **Contextual Relevance**:    - The\n",
            "retrieval component ensures that the generated content is contextually relevant,\n",
            "as it is based on the most pertinent information available.  3. **Scalability**:\n",
            "- RAG models can scale to handle vast amounts of data, making them suitable for\n",
            "applications requiring extensive knowledge bases.  4. **Versatility**:    -\n",
            "These models can be applied to a wide range of tasks, from question-answering\n",
            "and summarization to more complex dialogue systems.  ### Challenges of RAG  1.\n",
            "**Complexity**:    - Integrating retrieval and generation components adds\n",
            "complexity to the model architecture, making it more challenging to design,\n",
            "implement, and maintain.  2. **Computational Resources**:    - RAG models can be\n",
            "resource-intensive, requiring significant computational power for both the\n",
            "retrieval and generation processes.  3. **Latency**:    - The retrieval process\n",
            "can introduce latency, potentially slowing down the response time, which is\n",
            "critical for real-time applications.  4. **Quality of Retrieved Data**:    - The\n",
            "performance of the generative model heavily depends on the quality and relevance\n",
            "of the retrieved data. Poor retrieval can lead to suboptimal generation.  5.\n",
            "**Training Complexity**:    - Training RAG models can be more complex compared\n",
            "to standalone generative models, as it involves optimizing both retrieval and\n",
            "generation components simultaneously.  In summary, while Retrieval-Augmented\n",
            "Generation (RAG) offers substantial benefits in terms of accuracy, contextual\n",
            "relevance, scalability, and versatility, it also comes with its own set of\n",
            "challenges, including increased complexity, computational demands, potential\n",
            "latency issues, dependency on the quality of retrieved data, and training\n",
            "difficulties.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.Index-based search"
      ],
      "metadata": {
        "id": "t7djpPBpm0M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "wRarT_fym2XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3eba0d-e114-4408-f07d-c71e0fc65f6b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.216\n",
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(tfidf_df)\n",
        "\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ],
      "metadata": {
        "id": "SbokQ2eacHjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdccefc2-0d23-49d1-e86b-03e493051f90"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ability    access  accuracy  accurate    adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "1   0.000000  0.000000   0.00000  0.216814  0.00000    0.000000     0.000000   \n",
            "2   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "3   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "4   0.000000  0.000000   0.00000  0.000000  0.00000    0.236798     0.000000   \n",
            "5   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "6   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "7   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "8   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "9   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "10  0.186722  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "11  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "12  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "13  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "14  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "15  0.000000  0.172817   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "16  0.000000  0.318332   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "17  0.000000  0.000000   0.00000  0.207376  0.00000    0.000000     0.000000   \n",
            "18  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "19  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "20  0.000000  0.000000   0.00000  0.000000  0.27489    0.000000     0.000000   \n",
            "21  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "22  0.000000  0.174434   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "23  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "24  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "25  0.000000  0.000000   0.22915  0.000000  0.00000    0.000000     0.000000   \n",
            "26  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.173873   \n",
            "\n",
            "      advice  algorithms    allows  ...      vast   vector      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "7   0.000000    0.221347  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.294228   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.000000  0.22394  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "19  0.244567    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291828  ...  0.000000  0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.173873  0.00000  0.000000   \n",
            "\n",
            "       where     which     while      wide      with    within   without  \n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.260831  0.000000  \n",
            "1   0.000000  0.000000  0.000000  0.000000  0.160002  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.245200  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179186  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.181285  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "15  0.189693  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.000000  0.000000  0.257860  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "19  0.217317  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.191365  0.000000  0.291828  \n",
            "22  0.000000  0.000000  0.000000  0.215477  0.141298  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.329094  0.000000  0.215802  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.133742  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[27 rows x 292 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented input"
      ],
      "metadata": {
        "id": "dABZ12Bkugtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ],
      "metadata": {
        "id": "1w4wppuA4eNn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "MNozI65K4e7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537fede3-db92-4446-9c11-2792617e1b83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store While Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "hU998zkD4hpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "id": "uy9X-l_Iugtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaaa7b8c-2d13-467d-920b-51f87e286060"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ---\n",
            "**Define a RAG (Retrieval-Augmented Generation)**  **While Retrieval-Augmented\n",
            "Generation (RAG) offers substantial benefits, it also comes with its\n",
            "challenges.**  ---  ### What is Retrieval-Augmented Generation (RAG)?\n",
            "**Retrieval-Augmented Generation (RAG)** is a hybrid approach in Natural\n",
            "Language Processing (NLP) that combines the strengths of retrieval-based methods\n",
            "and generative models.   - **Retrieval-based methods** involve searching a large\n",
            "corpus of text to find relevant information or documents that can help answer a\n",
            "query. - **Generative models** are capable of producing new text based on the\n",
            "input they receive, often using techniques like sequence-to-sequence learning.\n",
            "In RAG, the system first retrieves relevant documents or pieces of information\n",
            "from a large dataset and then uses a generative model to produce a coherent and\n",
            "contextually appropriate response based on the retrieved information.  ###\n",
            "Benefits of RAG  1. **Enhanced Accuracy**: By leveraging a large corpus of pre-\n",
            "existing information, RAG can provide more accurate and contextually relevant\n",
            "responses. 2. **Knowledge Integration**: It can integrate specific, up-to-date\n",
            "information from external sources, making the generated content more informative\n",
            "and reliable. 3. **Flexibility**: RAG can handle a wide range of queries, from\n",
            "simple factual questions to more complex, open-ended ones. 4. **Efficiency**:\n",
            "Combining retrieval and generation can be more efficient than relying solely on\n",
            "generative models, especially for queries that require specific information.\n",
            "### Challenges of RAG  1. **Complexity**: Implementing a RAG system is more\n",
            "complex than using either retrieval-based or generative models alone. It\n",
            "requires careful integration and tuning of both components. 2. **Latency**: The\n",
            "retrieval step can introduce additional latency, making the system slower\n",
            "compared to purely generative models. 3. **Quality of Retrieved Data**: The\n",
            "quality of the generated response heavily depends on the relevance and quality\n",
            "of the retrieved documents. Poor retrieval can lead to inaccurate or irrelevant\n",
            "responses. 4. **Scalability**: Managing and searching through large datasets can\n",
            "be computationally expensive and challenging to scale. 5. **Bias and Fairness**:\n",
            "The system can inherit biases present in the retrieved documents, which can\n",
            "affect the fairness and neutrality of the generated responses.  ### Conclusion\n",
            "While Retrieval-Augmented Generation (RAG) offers substantial benefits by\n",
            "combining the strengths of retrieval-based methods and generative models, it\n",
            "also presents several challenges that need to be addressed. These include\n",
            "increased complexity, potential latency issues, dependency on the quality of\n",
            "retrieved data, scalability concerns, and the risk of bias. Despite these\n",
            "challenges, RAG remains a powerful approach in the field of NLP, capable of\n",
            "producing highly accurate and contextually relevant responses.  ---  I hope this\n",
            "elaboration helps you understand the concept of Retrieval-Augmented Generation\n",
            "(RAG) and its associated benefits and challenges!\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Modular RAG Retriever"
      ],
      "metadata": {
        "id": "wWEvzcDHTX6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ],
      "metadata": {
        "id": "18wmqwJd4o62"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modular RAG Strategies"
      ],
      "metadata": {
        "id": "7qHm4saJ8cGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "_kvhIOdY8amp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32f349f-a59b-41d9-cab3-fe887c03a29b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented Input"
      ],
      "metadata": {
        "id": "5TaQa7Dc7JwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+best_matching_record"
      ],
      "metadata": {
        "id": "X-hKjhIU7Jwg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "ZhSO-fyZ7Jwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb402056-8777-4224-8b32-9534076854f8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeWhile Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "qkyYx_MC7Jwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "id": "-V3srRHW7Jwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0568a4-0b15-4a20-e692-b0dd5dccbe8a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ---\n",
            "**Define a RAG (Retrieval-Augmented Generation)**  **Retrieval-Augmented\n",
            "Generation (RAG)** is a hybrid approach in natural language processing (NLP)\n",
            "that combines the strengths of retrieval-based methods and generation-based\n",
            "methods to improve the quality and relevance of generated text. Here’s a\n",
            "detailed explanation of the concept and its implications:  ### What is\n",
            "Retrieval-Augmented Generation (RAG)?  1. **Retrieval Component**:    -\n",
            "**Purpose**: The retrieval component is designed to fetch relevant information\n",
            "from a large corpus of documents or a database.    - **Mechanism**: It uses\n",
            "techniques like TF-IDF, BM25, or more advanced neural retrieval models to\n",
            "identify and retrieve the most relevant pieces of information based on the input\n",
            "query.    - **Example**: If the input query is \"What are the benefits of RAG?\",\n",
            "the retrieval component might fetch documents or passages that discuss the\n",
            "advantages of RAG.  2. **Generation Component**:    - **Purpose**: The\n",
            "generation component is responsible for producing coherent and contextually\n",
            "appropriate text based on the retrieved information.    - **Mechanism**: It\n",
            "typically uses advanced language models like GPT-3, BERT, or T5 to generate text\n",
            "that incorporates the retrieved information.    - **Example**: Using the\n",
            "information retrieved about the benefits of RAG, the generation component would\n",
            "construct a well-formed response that integrates this information.  ### Benefits\n",
            "of RAG  1. **Enhanced Relevance**:    - By leveraging a retrieval mechanism, RAG\n",
            "ensures that the generated text is grounded in actual, relevant information,\n",
            "leading to more accurate and contextually appropriate responses.  2. **Improved\n",
            "Knowledge Utilization**:    - RAG can dynamically access and utilize a vast\n",
            "amount of external knowledge, making it particularly useful for tasks that\n",
            "require up-to-date or specialized information.  3. **Reduced Hallucination**:\n",
            "- One of the common issues with purely generative models is the tendency to\n",
            "\"hallucinate\" or produce plausible-sounding but incorrect information. By\n",
            "grounding the generation in retrieved documents, RAG reduces the likelihood of\n",
            "such errors.  ### Challenges of RAG  1. **Complexity**:    - Integrating\n",
            "retrieval and generation components adds complexity to the system. It requires\n",
            "careful tuning and coordination between the two components to ensure seamless\n",
            "operation.  2. **Latency**:    - The retrieval process can introduce additional\n",
            "latency, making the overall response time slower compared to purely generative\n",
            "models. This can be a concern in real-time applications.  3. **Quality of\n",
            "Retrieved Information**:    - The effectiveness of RAG heavily depends on the\n",
            "quality and relevance of the retrieved information. If the retrieval component\n",
            "fetches irrelevant or low-quality documents, the generated text will also\n",
            "suffer.  4. **Scalability**:    - Managing and searching through large corpora\n",
            "of documents efficiently can be challenging, especially as the size of the\n",
            "corpus grows.  ### Conclusion  While Retrieval-Augmented Generation (RAG) offers\n",
            "substantial benefits in terms of relevance, knowledge utilization, and reduced\n",
            "hallucination, it also comes with its own set of challenges, including increased\n",
            "complexity, potential latency issues, dependency on the quality of retrieved\n",
            "information, and scalability concerns. Balancing these factors is crucial for\n",
            "the effective implementation of RAG in various NLP applications.  ---  I hope\n",
            "this detailed explanation helps you understand the concept of Retrieval-\n",
            "Augmented Generation (RAG) and its associated benefits and challenges!\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}