{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqpsYn49QWSR"
      },
      "source": [
        "#Introducing Naïve, Advanced, and Modular RAG\n",
        "\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "This notebook introduces Naïve, Advanced, and Modular RAG through basic educational examples.\n",
        "\n",
        "It explores keyword matching, vector search, and index-based retrieval methods. Using OpenAI's GPT models, it generates responses based on input queries and retrieved documents.\n",
        "\n",
        "The modular RAG system offers flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "- Introduction of Naïve, Advanced, and Modular RAG\n",
        "- Environment setup for OpenAI API integration\n",
        "- Generator function using GPT models\n",
        "- Formatted response printing\n",
        "- **Data** setup with a list of documents (db_records)\n",
        "- **Query** user request\n",
        "- **1.Naïve RAG**:\n",
        "  - Keyword search and matching function\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT\n",
        "- **2.Advanced RAG**:\n",
        "  - Vector search:\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "  - Index-based retrieval:\n",
        "    - Setup of TF-IDF vectorizer and matrix\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "- **3.Modular RAG Retriever**:\n",
        "  - RetrievalComponent class with methods for keyword, vector, and indexed search\n",
        "  - Usage example with different retrieval methods\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o01-IM8bTc5f"
      },
      "source": [
        "# The Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VCfbN0YwHbE",
        "outputId": "e753a02f-216d-4ae9-9cdb-a37384649562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.19.0\n",
            "  Downloading openai-1.19.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.19.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.19.0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.19.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.19.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (2.20.1)\n",
            "Downloading openai-1.19.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myXJn33zbqTR",
        "outputId": "4cd75e95-5989-474c-ed51-97f046e8e076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#API Key\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Oefvqp21Ba07"
      },
      "outputs": [],
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuZ7jr4Rs36U"
      },
      "source": [
        "# The Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qwCNTW9fs36U"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\"\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVKe9VF0HIHJ"
      },
      "source": [
        "### Formatted response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oG8I2Kb2HFhL"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv1ExPiZdJRL"
      },
      "source": [
        " # The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "45CFxG4Fgcju"
      },
      "outputs": [],
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FIQ7NK92g7EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c632605-22ec-4cf4-cef2-633baf298123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form. A RAG vector store is a database or\n",
            "dataset that contains vectorized data points.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL7cHuuLhQ5w"
      },
      "source": [
        "## The Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qARk6gtohSXW"
      },
      "outputs": [],
      "source": [
        "query = \"define a rag store\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4lLAvV6NIn_"
      },
      "source": [
        "### Generation without augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iO-k1Tq4MqmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1731796d-e2c6-41ab-a6e6-1eed41e51aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! It looks like the content you've provided is a sequence of characters\n",
            "that, when combined, spell out the phrase \"define a rag store.\" Let's break this\n",
            "down step by step:  1. **Characters Provided**:    - d    - e    - f    - i    -\n",
            "n    - e    - (space)    - a    - (space)    - r    - a    - g    - (space)    -\n",
            "s    - t    - o    - r    - e  2. **Combining Characters**:    When you combine\n",
            "these characters, you get the phrase \"define a rag store.\"  3. **Understanding\n",
            "the Phrase**:    - **Define**: This is a verb that means to explain the meaning\n",
            "of a word or concept.    - **a**: This is an indefinite article used before\n",
            "words that begin with a consonant sound.    - **rag**: This is a noun that\n",
            "typically refers to a piece of old, often torn, cloth used for cleaning or\n",
            "wiping.    - **store**: This is a noun that refers to a place where goods are\n",
            "sold to the public.  4. **Putting It All Together**:    The phrase \"define a rag\n",
            "store\" is essentially asking for an explanation or definition of what a \"rag\n",
            "store\" is.  5. **Defining a Rag Store**:    A \"rag store\" could be interpreted\n",
            "in a couple of ways:    - **Literal Interpretation**: A store that sells rags.\n",
            "This could be a shop that specializes in selling cleaning cloths, old fabrics,\n",
            "or recycled textiles.    - **Metaphorical Interpretation**: It could also be a\n",
            "colloquial or informal term for a second-hand store or thrift shop that sells\n",
            "used clothing and other items.  In summary, the content you provided spells out\n",
            "the phrase \"define a rag store,\" which is a request to explain what a rag store\n",
            "is. A rag store is typically a place where rags or old fabrics are sold, but it\n",
            "could also refer to a second-hand store selling various used items.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFqh9rr81SUn"
      },
      "source": [
        "# 1.Naïve Retrieval Augmented Generation(RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8vteKmS_qO"
      },
      "source": [
        "## Keyword search and matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WY1JU0Ush_l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84aef28a-af93-4678-ef44-38c5cd370b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 3\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zKQhiO0Fcr"
      },
      "source": [
        "## Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r_7ymSxG0Fcs"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+ \": \"+ best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7NTfxzum0PT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d6adbd-7056-4611-aad0-7a546d741e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ui8wH4k3_g4"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Jh8BsnUy0Fcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef77788-86ed-4a8d-dd79-bdaae7f4aec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "RAG Store:  A **RAG vector store** is a **database** or **dataset** that\n",
            "contains **vectorized data points**.  #### Key Components:  1. **RAG**:    -\n",
            "**RAG** stands for **Retrieval-Augmented Generation**. This is a technique used\n",
            "in natural language processing (NLP) where a model retrieves relevant\n",
            "information from a database or dataset to augment its generation capabilities.\n",
            "This helps in producing more accurate and contextually relevant responses.  2.\n",
            "**Vector Store**:    - A **vector store** is a specialized type of database\n",
            "designed to store and manage vectorized data points. Vectorized data points are\n",
            "numerical representations of data, often derived from text, images, or other\n",
            "forms of unstructured data using various embedding techniques.  3. **Database or\n",
            "Dataset**:    - The vector store can be implemented as a database or a dataset.\n",
            "A database is a structured collection of data that allows for efficient storage,\n",
            "retrieval, and management. A dataset is a collection of data, often used for\n",
            "training or testing machine learning models.  4. **Vectorized Data Points**:\n",
            "- **Vectorized data points** are numerical representations of data. In the\n",
            "context of text, these vectors are often generated using techniques like word\n",
            "embeddings (e.g., Word2Vec, GloVe) or sentence embeddings (e.g., BERT, GPT).\n",
            "These vectors capture the semantic meaning of the data, allowing for efficient\n",
            "similarity searches and other operations.  ### Detailed Explanation:  A **RAG\n",
            "vector store** plays a crucial role in modern NLP applications. Here's how it\n",
            "works:  1. **Data Vectorization**:    - Raw data (such as text, images, or other\n",
            "forms of unstructured data) is processed and converted into vectors. This\n",
            "process involves using machine learning models to generate embeddings that\n",
            "capture the semantic meaning of the data.  2. **Storage**:    - These vectors\n",
            "are then stored in a vector store, which is optimized for handling high-\n",
            "dimensional data. The vector store allows for efficient storage and retrieval of\n",
            "these vectors.  3. **Retrieval**:    - When a query is made, the RAG system\n",
            "retrieves relevant vectors from the vector store. This retrieval process often\n",
            "involves finding vectors that are similar to the query vector, using techniques\n",
            "like nearest neighbor search.  4. **Augmentation**:    - The retrieved vectors\n",
            "are used to augment the generation process. For example, in a text generation\n",
            "task, the retrieved vectors can provide additional context or information that\n",
            "helps the model generate more accurate and relevant responses.  5.\n",
            "**Generation**:    - The augmented information is then used by the generation\n",
            "model to produce the final output. This output is typically more accurate and\n",
            "contextually relevant compared to using the generation model alone.  ###\n",
            "Applications:  - **Question Answering**: Enhancing the accuracy of answers by\n",
            "retrieving relevant information from a large corpus. - **Chatbots**: Improving\n",
            "the contextual relevance of responses by augmenting the chatbot's knowledge\n",
            "base. - **Content Generation**: Assisting in generating more coherent and\n",
            "contextually appropriate content by leveraging external information.  In\n",
            "summary, a **RAG vector store** is a powerful tool in the field of NLP, enabling\n",
            "more accurate and contextually aware generation of responses by leveraging\n",
            "vectorized representations of data.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJH2__0iTUr1"
      },
      "source": [
        "# 2.Advanced Retrieval Augmented Generation(RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awyjcn35jFiy"
      },
      "source": [
        "## 2.1.Vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FCBbY4qLc8qh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english',\n",
        "        use_idf=True,\n",
        "        norm='l2',\n",
        "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
        "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
        "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
        "    )\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "RG1iM-U33OCg",
        "outputId": "15ba9ca4-3f53-4f6e-f2f5-99c2bba79bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.126\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced similarity"
      ],
      "metadata": {
        "id": "Dj1F9WndelkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    lemmatized_words = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        lemmatized_words.append(token.lemma_)\n",
        "    return lemmatized_words\n",
        "\n",
        "def expand_with_synonyms(words):\n",
        "    expanded_words = words.copy()\n",
        "    for word in words:\n",
        "        expanded_words.extend(get_synonyms(word))\n",
        "    return expanded_words\n",
        "\n",
        "def calculate_enhanced_similarity(text1, text2):\n",
        "    # Preprocess and tokenize texts\n",
        "    words1 = preprocess_text(text1)\n",
        "    words2 = preprocess_text(text2)\n",
        "\n",
        "    # Expand with synonyms\n",
        "    words1_expanded = expand_with_synonyms(words1)\n",
        "    words2_expanded = expand_with_synonyms(words2)\n",
        "\n",
        "    # Count word frequencies\n",
        "    freq1 = Counter(words1_expanded)\n",
        "    freq2 = Counter(words2_expanded)\n",
        "\n",
        "    # Create a set of all unique words\n",
        "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
        "\n",
        "    # Create frequency vectors\n",
        "    vector1 = [freq1[word] for word in unique_words]\n",
        "    vector2 = [freq2[word] for word in unique_words]\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(\"Enhanced Similarity:\", similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iquYhckxYCLa",
        "outputId": "2e5687a5-ef4b-4366-9717-e62032f6982e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity: 0.641582812483307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fZC6Oe2G9E"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4dcnK7OGx5e6"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T3uk-91x049J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444db14b-75a9-4865-d29f-b3b39768de97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFDF6hbi2LF9"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YJC-mA5ftxFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4616961-a320-482f-8f80-d8535124f07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "RAG Store:  **RAG** stands for **Retrieval-Augmented Generation**. A **RAG\n",
            "store** is a specialized type of database or dataset that is designed to store\n",
            "and manage vectorized data points.   #### Key Concepts:  1. **Vectorized Data\n",
            "Points**:    - **Vectorization** is the process of converting data into a\n",
            "numerical format that can be easily processed by machine learning algorithms. In\n",
            "the context of text data, this often involves converting words, sentences, or\n",
            "documents into vectors (arrays of numbers) that capture the semantic meaning of\n",
            "the text.    - These vectors are typically generated using techniques such as\n",
            "word embeddings (e.g., Word2Vec, GloVe) or more advanced models like BERT or\n",
            "GPT.  2. **Database or Dataset**:    - A **database** is an organized collection\n",
            "of data, generally stored and accessed electronically from a computer system. In\n",
            "the context of a RAG store, the database is optimized to handle vectorized data\n",
            "efficiently.    - A **dataset** refers to a collection of data points that are\n",
            "used for training or evaluating machine learning models. In a RAG store, the\n",
            "dataset would consist of vectorized representations of the data.  3.\n",
            "**Retrieval-Augmented Generation (RAG)**:    - **Retrieval-Augmented\n",
            "Generation** is a technique used in natural language processing where a model\n",
            "retrieves relevant information from a large dataset (the RAG store) to help\n",
            "generate more accurate and contextually appropriate responses.    - This\n",
            "approach combines the strengths of retrieval-based models (which can access a\n",
            "vast amount of information) with the generative capabilities of models like\n",
            "GPT-3, resulting in more informed and coherent outputs.  #### Applications of a\n",
            "RAG Store:  - **Question Answering Systems**: A RAG store can be used to store a\n",
            "large corpus of documents. When a question is asked, the system retrieves\n",
            "relevant documents from the RAG store and uses them to generate a precise\n",
            "answer. - **Chatbots**: Chatbots can use a RAG store to access a wide range of\n",
            "information, allowing them to provide more accurate and contextually relevant\n",
            "responses to user queries. - **Content Generation**: For tasks like\n",
            "summarization or content creation, a RAG store can provide the necessary\n",
            "background information to generate high-quality content.  #### Example Workflow:\n",
            "1. **Data Collection**: Gather a large corpus of text data. 2.\n",
            "**Vectorization**: Convert the text data into vectorized representations using\n",
            "an appropriate model. 3. **Storage**: Store these vectors in a RAG store (a\n",
            "specialized database). 4. **Retrieval**: When a query is made, retrieve the most\n",
            "relevant vectors from the RAG store. 5. **Generation**: Use the retrieved\n",
            "vectors to generate a response or output.  In summary, a RAG store is a powerful\n",
            "tool in the field of natural language processing, enabling systems to leverage\n",
            "vast amounts of vectorized data to enhance the accuracy and relevance of\n",
            "generated content.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7djpPBpm0M2"
      },
      "source": [
        "## 2.2.Index-based search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wRarT_fym2XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9117f336-1032-477f-c87b-24aa68958f17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.407\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced similarity"
      ],
      "metadata": {
        "id": "vM3zsSB2ehR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    lemmatized_words = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        lemmatized_words.append(token.lemma_)\n",
        "    return lemmatized_words\n",
        "\n",
        "def expand_with_synonyms(words):\n",
        "    expanded_words = words.copy()\n",
        "    for word in words:\n",
        "        expanded_words.extend(get_synonyms(word))\n",
        "    return expanded_words\n",
        "\n",
        "def calculate_enhanced_similarity(text1, text2):\n",
        "    # Preprocess and tokenize texts\n",
        "    words1 = preprocess_text(text1)\n",
        "    words2 = preprocess_text(text2)\n",
        "\n",
        "    # Expand with synonyms\n",
        "    words1_expanded = expand_with_synonyms(words1)\n",
        "    words2_expanded = expand_with_synonyms(words2)\n",
        "\n",
        "    # Count word frequencies\n",
        "    freq1 = Counter(words1_expanded)\n",
        "    freq2 = Counter(words2_expanded)\n",
        "\n",
        "    # Create a set of all unique words\n",
        "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
        "\n",
        "    # Create frequency vectors\n",
        "    vector1 = [freq1[word] for word in unique_words]\n",
        "    vector2 = [freq2[word] for word in unique_words]\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "response = best_matching_record\n",
        "print(query, \":\",response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(\"Enhanced Similarity:\", similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e737ae8-9881-4a5e-fffd-70c308f48654",
        "id": "NDcxQNvOcneS"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store : A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity: 0.641582812483307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature extraction"
      ],
      "metadata": {
        "id": "ubm0DTxKeqR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SbokQ2eacHjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040185ad-7038-4662-f8a5-c744c0e70ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
            "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "\n",
            "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
            "\n",
            "       where     which    while     wide      with    within   without  \n",
            "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
            "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
            "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[28 rows x 297 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(tfidf_df)\n",
        "\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dABZ12Bkugtt"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1w4wppuA4eNn"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MNozI65K4e7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19115e2-1cc7-43d9-afec-1ecaa019d21f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU998zkD4hpD"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uy9X-l_Iugtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd01f38f-e58b-4683-9ab0-589763c5dc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "RAG Store:  A **RAG vector store** is a **database** or **dataset** that\n",
            "contains **vectorized data points**.  #### Key Concepts:  1. **RAG (Retrieval-\n",
            "Augmented Generation)**:    - RAG is a technique used in natural language\n",
            "processing (NLP) that combines retrieval-based methods with generation-based\n",
            "methods.    - It involves retrieving relevant information from a large dataset\n",
            "and then using that information to generate a response or output.  2. **Vector\n",
            "Store**:    - A vector store is a specialized database designed to store and\n",
            "manage vectorized data points.    - Vectorized data points are numerical\n",
            "representations of data, often used in machine learning and NLP to represent\n",
            "text, images, or other types of data in a format that can be easily processed by\n",
            "algorithms.  3. **Vectorized Data Points**:    - These are data points that have\n",
            "been transformed into vectors, which are arrays of numbers.    - For example, in\n",
            "NLP, words or sentences can be converted into vectors using techniques like word\n",
            "embeddings (e.g., Word2Vec, GloVe) or contextual embeddings (e.g., BERT, GPT).\n",
            "- These vectors capture the semantic meaning of the data and can be used for\n",
            "various tasks such as similarity search, clustering, and classification.  ####\n",
            "How a RAG Vector Store Works:  1. **Data Storage**:    - The vector store holds\n",
            "a large number of vectorized data points. These could be vectors representing\n",
            "text documents, images, or other types of data.    - The vectors are typically\n",
            "stored in a way that allows for efficient retrieval and similarity search.  2.\n",
            "**Retrieval**:    - When a query is made, the system retrieves the most relevant\n",
            "vectors from the store.    - This retrieval process often involves calculating\n",
            "the similarity between the query vector and the vectors in the store using\n",
            "metrics like cosine similarity or Euclidean distance.  3. **Augmentation**:    -\n",
            "The retrieved vectors are then used to augment the input data.    - In the\n",
            "context of NLP, this might involve using the retrieved information to provide\n",
            "context or additional details that can help generate a more accurate or\n",
            "informative response.  4. **Generation**:    - Finally, a generation model\n",
            "(e.g., a transformer-based model like GPT) uses the augmented data to produce\n",
            "the final output.    - This output could be a text response, a summary, or any\n",
            "other type of generated content.  #### Applications:  - **Question Answering**:\n",
            "RAG can be used to answer questions by retrieving relevant documents and\n",
            "generating answers based on the retrieved information. - **Chatbots**: Enhancing\n",
            "chatbot responses by retrieving relevant context from a large dataset. -\n",
            "**Content Generation**: Generating articles, summaries, or other content by\n",
            "leveraging a large corpus of vectorized data.  In summary, a RAG vector store is\n",
            "a powerful tool in NLP that combines the strengths of retrieval-based and\n",
            "generation-based methods to produce more accurate and contextually relevant\n",
            "outputs. It leverages vectorized representations of data to efficiently store,\n",
            "retrieve, and utilize information.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWEvzcDHTX6i"
      },
      "source": [
        "# 3.Modular RAG Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "18wmqwJd4o62"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHm4saJ8cGk"
      },
      "source": [
        "### Modular RAG Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_kvhIOdY8amp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d18ade-f736-485e-e3e8-bd4500714182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TaQa7Dc7JwT"
      },
      "source": [
        "### Augmented Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "X-hKjhIU7Jwg"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+ \" \"+ best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZhSO-fyZ7Jwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06319d1c-31a1-4306-e557-4bd249c499d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkyYx_MC7Jwg"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-V3srRHW7Jwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a26ec2-6dc5-4c6a-fcfe-643fc49fd4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "RAG Store  A **RAG (Retrieval-Augmented Generation) store** is a specialized\n",
            "database or dataset designed to store and manage vectorized data points. This\n",
            "type of store is particularly useful in applications involving machine learning\n",
            "and natural language processing (NLP).  ### What is a Vector Store?  A **vector\n",
            "store** is a database or dataset that contains data points represented as\n",
            "vectors. Vectors are mathematical representations of data that capture the\n",
            "essential features and relationships of the data in a multi-dimensional space.\n",
            "In the context of NLP, vectors often represent words, sentences, or documents in\n",
            "a way that preserves their semantic meaning.  ### Key Components and Functions\n",
            "of a RAG Store  1. **Vectorization**:    - The process of converting raw data\n",
            "(such as text, images, or other forms of data) into vectors. This is typically\n",
            "done using machine learning models like word embeddings (e.g., Word2Vec, GloVe)\n",
            "or more advanced models like BERT or GPT.  2. **Storage**:    - The vectors are\n",
            "stored in a database that allows for efficient retrieval and manipulation. This\n",
            "database can be optimized for handling high-dimensional data and performing\n",
            "operations like similarity searches.  3. **Retrieval**:    - One of the primary\n",
            "functions of a RAG store is to retrieve relevant vectors based on a query. This\n",
            "is often done using similarity measures like cosine similarity, which helps in\n",
            "finding vectors that are close to the query vector in the multi-dimensional\n",
            "space.  4. **Augmentation**:    - In the context of RAG, the retrieved vectors\n",
            "are used to augment the generation process. For example, in a text generation\n",
            "task, the retrieved vectors can provide additional context or information that\n",
            "helps in generating more accurate and relevant responses.  ### Applications of a\n",
            "RAG Store  1. **Information Retrieval**:    - Enhancing search engines by\n",
            "retrieving documents or snippets that are semantically similar to the query.  2.\n",
            "**Question Answering Systems**:    - Improving the accuracy of answers by\n",
            "retrieving relevant information from a large corpus of text.  3.\n",
            "**Recommendation Systems**:    - Providing personalized recommendations by\n",
            "finding items that are similar to the user's preferences.  4. **Text\n",
            "Generation**:    - Augmenting the generation of text by incorporating relevant\n",
            "information retrieved from the vector store.  ### Example Scenario  Imagine you\n",
            "have a large collection of research papers. You want to build a system that can\n",
            "answer questions based on the content of these papers. Here's how a RAG store\n",
            "would be useful:  1. **Vectorization**:    - Convert the text of each research\n",
            "paper into vectors using a pre-trained language model.  2. **Storage**:    -\n",
            "Store these vectors in a vector store, indexed for efficient retrieval.  3.\n",
            "**Retrieval**:    - When a user asks a question, convert the question into a\n",
            "vector and retrieve the most similar vectors (research papers) from the store.\n",
            "4. **Augmentation**:    - Use the retrieved vectors to generate a detailed and\n",
            "accurate answer to the user's question.  In summary, a RAG store is a powerful\n",
            "tool for managing and utilizing vectorized data points, enabling advanced\n",
            "applications in NLP and machine learning by combining efficient retrieval with\n",
            "generation capabilities.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNuprspAV1vPK4jDHlpe9Z6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}