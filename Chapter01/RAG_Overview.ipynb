{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNC4JrDPEyKEehPzsyO6nHt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introducing Naïve, Advanced, and Modular RAG\n",
        "\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "This notebook introduces Naïve, Advanced, and Modular RAG through basic educational examples.\n",
        "\n",
        "It explores keyword matching, vector search, and index-based retrieval methods. Using OpenAI's GPT models, it generates responses based on input queries and retrieved documents.\n",
        "\n",
        "The modular RAG system offers flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "- Introduction of Naïve, Advanced, and Modular RAG\n",
        "- Environment setup for OpenAI API integration\n",
        "- Generator function using GPT models\n",
        "- Formatted response printing\n",
        "- **Data** setup with a list of documents (db_records)\n",
        "- **Query** user request\n",
        "- **1.Naïve RAG**:\n",
        "  - Keyword search and matching function\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT\n",
        "- **2.Advanced RAG**:\n",
        "  - Vector search:\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "  - Index-based retrieval:\n",
        "    - Setup of TF-IDF vectorizer and matrix\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "- **3.Modular RAG Retriever**:\n",
        "  - RetrievalComponent class with methods for keyword, vector, and indexed search\n",
        "  - Usage example with different retrieval methods\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT"
      ],
      "metadata": {
        "id": "sqpsYn49QWSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Environment"
      ],
      "metadata": {
        "id": "o01-IM8bTc5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.19.0"
      ],
      "metadata": {
        "id": "8VCfbN0YwHbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eac169d-bf8b-4aec-8d25-fb1a472b0aa3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.19.0 in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.19.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.19.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#API Key\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe79b29a-bdf0-4bd1-9492-fc49fc028ecb",
        "id": "myXJn33zbqTR"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "Oefvqp21Ba07"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Generator\n"
      ],
      "metadata": {
        "id": "WuZ7jr4Rs36U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\"\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ],
      "metadata": {
        "id": "qwCNTW9fs36U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formatted response"
      ],
      "metadata": {
        "id": "bVKe9VF0HIHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ],
      "metadata": {
        "id": "oG8I2Kb2HFhL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # The Data"
      ],
      "metadata": {
        "id": "Qv1ExPiZdJRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\"\n",
        "]"
      ],
      "metadata": {
        "id": "45CFxG4Fgcju"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "FIQ7NK92g7EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fc0c95-393e-4e46-978f-e548356f224b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Query"
      ],
      "metadata": {
        "id": "aL7cHuuLhQ5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"define a rag store\""
      ],
      "metadata": {
        "id": "qARk6gtohSXW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation without augmentation"
      ],
      "metadata": {
        "id": "U4lLAvV6NIn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "id": "iO-k1Tq4MqmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b953a20-ca59-463d-c8d1-cba9f0f8af4d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! The content you've provided appears to be a sequence of characters\n",
            "that, when combined, form the phrase \"define a rag store.\" Let's break it down\n",
            "step by step:  1. **Characters and Formation**:    - The sequence of characters\n",
            "is:      ```      d      e      f      i      n      e            a            r\n",
            "a      g            s      t      o      r      e      ```    - When these\n",
            "characters are concatenated, they form the phrase \"define a rag store.\"  2.\n",
            "**Phrase Analysis**:    - **Define**: This is a verb that means to explain the\n",
            "meaning of a word or concept.    - **a**: This is an indefinite article used\n",
            "before words that begin with a consonant sound.    - **rag**: This is a noun\n",
            "that typically refers to a piece of old, often torn, cloth.    - **store**: This\n",
            "is a noun that refers to a place where goods are sold to the public.  3.\n",
            "**Combined Meaning**:    - When combined, the phrase \"define a rag store\" is a\n",
            "request or instruction to explain what a \"rag store\" is.  4. **Possible\n",
            "Interpretation of \"Rag Store\"**:    - A \"rag store\" could be interpreted as a\n",
            "shop that sells rags or old pieces of cloth. These stores might sell rags for\n",
            "cleaning, crafting, or other purposes.    - It could also be a colloquial term\n",
            "for a second-hand store or thrift shop that sells used clothing and other items.\n",
            "5. **Contextual Usage**:    - The phrase \"define a rag store\" might be used in\n",
            "various contexts, such as:      - A dictionary or glossary entry where someone\n",
            "is asked to provide the definition of a rag store.      - A classroom or\n",
            "educational setting where a teacher asks students to explain what a rag store\n",
            "is.      - A business context where someone is describing the nature of their\n",
            "store or business.  By breaking down the sequence of characters and analyzing\n",
            "the phrase, we can understand that it is a request to explain what a \"rag store\"\n",
            "is, and we can infer that a rag store is likely a shop that sells rags or\n",
            "second-hand items.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Naïve Retrieval Augmented Generation(RAG)"
      ],
      "metadata": {
        "id": "JFqh9rr81SUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword search and matching"
      ],
      "metadata": {
        "id": "Wu8vteKmS_qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "WY1JU0Ush_l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decca6d5-f977-4fcd-9add-0b58cfca1bc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 1\n",
            "Response:\n",
            "---------------\n",
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmented input"
      ],
      "metadata": {
        "id": "r2zKQhiO0Fcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+best_matching_record"
      ],
      "metadata": {
        "id": "r_7ymSxG0Fcs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "7NTfxzum0PT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5091b07b-5fce-4783-a39f-a36257d830cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeRetrieval Augmented Generation (RAG) represents a\n",
            "sophisticated hybrid approach in the field of artificial intelligence,\n",
            "particularly within the realm of natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "8Ui8wH4k3_g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "id": "Jh8BsnUy0Fcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552570f7-7cf7-4b99-c576-70ecaad1b051"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the provided content:  ---\n",
            "**Define a RAG (Retrieval-Augmented Generation) represents a sophisticated\n",
            "hybrid approach in the field of artificial intelligence, particularly within the\n",
            "realm of natural language processing (NLP).**  ### Explanation:  **1. Retrieval-\n",
            "Augmented Generation (RAG):**    - **Definition:** RAG is a technique that\n",
            "combines two powerful methods in artificial intelligence: retrieval-based\n",
            "methods and generation-based methods.    - **Purpose:** The goal of RAG is to\n",
            "enhance the capabilities of AI systems, particularly in generating more accurate\n",
            "and contextually relevant responses.  **2. Sophisticated Hybrid Approach:**    -\n",
            "**Sophisticated:** This implies that RAG is an advanced and complex method,\n",
            "leveraging the strengths of multiple techniques.    - **Hybrid Approach:** RAG\n",
            "integrates both retrieval and generation methods, creating a system that can\n",
            "retrieve relevant information and generate coherent responses based on that\n",
            "information.  **3. Field of Artificial Intelligence (AI):**    - **AI:** This is\n",
            "a broad field that involves creating systems capable of performing tasks that\n",
            "typically require human intelligence, such as understanding natural language,\n",
            "recognizing patterns, and making decisions.  **4. Natural Language Processing\n",
            "(NLP):**    - **NLP:** This is a subfield of AI focused on the interaction\n",
            "between computers and humans through natural language. It involves enabling\n",
            "machines to understand, interpret, and generate human language in a way that is\n",
            "both meaningful and useful.  ### How RAG Works:  1. **Retrieval Component:**\n",
            "- **Function:** The retrieval component searches a large database or corpus to\n",
            "find relevant documents or pieces of information based on a given query or\n",
            "context.    - **Example:** If the system is asked a question, the retrieval\n",
            "component will look for documents or text passages that contain relevant\n",
            "information to answer the question.  2. **Generation Component:**    -\n",
            "**Function:** The generation component takes the retrieved information and uses\n",
            "it to generate a coherent and contextually appropriate response.    -\n",
            "**Example:** Using the information retrieved, the generation component\n",
            "constructs a detailed and accurate answer to the question.  ### Applications of\n",
            "RAG in NLP:  - **Question Answering Systems:** RAG can be used to build systems\n",
            "that provide accurate answers to user queries by retrieving relevant information\n",
            "and generating precise responses. - **Chatbots:** Enhancing chatbots to provide\n",
            "more contextually relevant and informative replies by combining retrieval of\n",
            "relevant data with natural language generation. - **Content Creation:**\n",
            "Assisting in generating content by retrieving relevant information and using it\n",
            "to create coherent and contextually appropriate text.  ### Benefits of RAG:  -\n",
            "**Improved Accuracy:** By combining retrieval and generation, RAG systems can\n",
            "provide more accurate and contextually relevant responses. - **Enhanced Context\n",
            "Understanding:** The retrieval component ensures that the generated responses\n",
            "are based on relevant and up-to-date information. - **Versatility:** RAG can be\n",
            "applied to various NLP tasks, making it a versatile approach in the field of AI.\n",
            "---  In summary, Retrieval-Augmented Generation (RAG) is an advanced method in\n",
            "AI that combines the strengths of retrieval-based and generation-based\n",
            "approaches to enhance the capabilities of natural language processing systems.\n",
            "This hybrid approach allows for more accurate, contextually relevant, and\n",
            "coherent responses in various applications, such as question answering and\n",
            "chatbots.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Advanced Retrieval Augmented Generation(RAG)"
      ],
      "metadata": {
        "id": "zJH2__0iTUr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.Vector search"
      ],
      "metadata": {
        "id": "awyjcn35jFiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record\n",
        "\n",
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "FCBbY4qLc8qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d63ca7e-aee5-462d-f15c-cea56fbea1a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.087\n",
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented input"
      ],
      "metadata": {
        "id": "51fZC6Oe2G9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ],
      "metadata": {
        "id": "4dcnK7OGx5e6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "T3uk-91x049J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276b5987-7317-47f8-adb8-31e0fed0ef02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store While Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "wFDF6hbi2LF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "id": "YJC-mA5ftxFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28567170-03c9-43ee-a13e-8fd3cc64c379"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ---\n",
            "**Define a RAG (Retrieval-Augmented Generation)**  **While Retrieval-Augmented\n",
            "Generation (RAG) offers substantial benefits, it also comes with its\n",
            "challenges.**  ---  ### What is Retrieval-Augmented Generation (RAG)?\n",
            "**Retrieval-Augmented Generation (RAG)** is a hybrid approach in Natural\n",
            "Language Processing (NLP) that combines the strengths of retrieval-based methods\n",
            "and generative models. Here's a detailed explanation:  1. **Retrieval-Based\n",
            "Methods**:    - These methods involve searching a large corpus of text to find\n",
            "relevant documents or passages that can help answer a query.    - They are\n",
            "efficient and can provide accurate information by leveraging existing knowledge.\n",
            "2. **Generative Models**:    - These models, such as GPT-3, generate text based\n",
            "on the input they receive.    - They are capable of producing coherent and\n",
            "contextually relevant responses, even for queries that do not have direct\n",
            "answers in the corpus.  **RAG** combines these two approaches by first\n",
            "retrieving relevant documents from a large corpus and then using a generative\n",
            "model to produce a response based on the retrieved information. This allows the\n",
            "system to generate more accurate and contextually appropriate answers.  ###\n",
            "Benefits of RAG  1. **Enhanced Accuracy**:    - By leveraging a large corpus of\n",
            "text, RAG can provide more accurate and contextually relevant responses.    -\n",
            "The retrieval step ensures that the generative model has access to the most\n",
            "relevant information.  2. **Improved Knowledge Coverage**:    - RAG can handle a\n",
            "wide range of topics by retrieving information from diverse sources.    - This\n",
            "makes it suitable for applications requiring extensive knowledge, such as\n",
            "customer support or educational tools.  3. **Contextual Relevance**:    - The\n",
            "generative model can produce responses that are not only accurate but also\n",
            "contextually appropriate.    - This improves the user experience by providing\n",
            "more natural and coherent answers.  ### Challenges of RAG  1. **Complexity**:\n",
            "- Implementing a RAG system is more complex than using either retrieval-based\n",
            "methods or generative models alone.    - It requires careful integration of both\n",
            "components to ensure seamless operation.  2. **Computational Resources**:    -\n",
            "RAG systems can be resource-intensive, requiring significant computational power\n",
            "for both retrieval and generation.    - This can be a barrier for organizations\n",
            "with limited resources.  3. **Latency**:    - The retrieval step can introduce\n",
            "latency, making the response time longer compared to using a generative model\n",
            "alone.    - Optimizing the retrieval process is crucial to minimize delays.  4.\n",
            "**Quality of Retrieved Information**:    - The quality of the generated response\n",
            "heavily depends on the quality of the retrieved documents.    - If the retrieval\n",
            "step brings back irrelevant or low-quality information, the generative model's\n",
            "output will be affected.  5. **Maintenance and Updates**:    - Keeping the\n",
            "corpus up-to-date and relevant is an ongoing challenge.    - Regular updates and\n",
            "maintenance are required to ensure the system remains effective.  ### Conclusion\n",
            "While Retrieval-Augmented Generation (RAG) offers substantial benefits in terms\n",
            "of accuracy, knowledge coverage, and contextual relevance, it also comes with\n",
            "its own set of challenges, including complexity, computational resource\n",
            "requirements, latency, quality of retrieved information, and maintenance.\n",
            "Balancing these benefits and challenges is key to successfully implementing and\n",
            "leveraging RAG in various applications.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.Index-based search"
      ],
      "metadata": {
        "id": "t7djpPBpm0M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "wRarT_fym2XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5695100-30fb-47a3-be96-1fc718fcb83c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.216\n",
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(tfidf_df)\n",
        "\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ],
      "metadata": {
        "id": "SbokQ2eacHjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d6cdf6-5a1f-407d-80ef-df119ea2571e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ability    access  accuracy  accurate    adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "1   0.000000  0.000000   0.00000  0.216814  0.00000    0.000000     0.000000   \n",
            "2   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "3   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "4   0.000000  0.000000   0.00000  0.000000  0.00000    0.236798     0.000000   \n",
            "5   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "6   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "7   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "8   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "9   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "10  0.186722  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "11  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "12  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "13  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "14  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "15  0.000000  0.172817   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "16  0.000000  0.318332   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "17  0.000000  0.000000   0.00000  0.207376  0.00000    0.000000     0.000000   \n",
            "18  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "19  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "20  0.000000  0.000000   0.00000  0.000000  0.27489    0.000000     0.000000   \n",
            "21  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "22  0.000000  0.174434   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "23  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "24  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "25  0.000000  0.000000   0.22915  0.000000  0.00000    0.000000     0.000000   \n",
            "26  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.173873   \n",
            "\n",
            "      advice  algorithms    allows  ...      vast   vector      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "7   0.000000    0.221347  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.294228   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.000000  0.22394  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "19  0.244567    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291828  ...  0.000000  0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.173873  0.00000  0.000000   \n",
            "\n",
            "       where     which     while      wide      with    within   without  \n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.260831  0.000000  \n",
            "1   0.000000  0.000000  0.000000  0.000000  0.160002  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.245200  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179186  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.181285  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "15  0.189693  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.000000  0.000000  0.257860  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "19  0.217317  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.191365  0.000000  0.291828  \n",
            "22  0.000000  0.000000  0.000000  0.215477  0.141298  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.329094  0.000000  0.215802  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.133742  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[27 rows x 292 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented input"
      ],
      "metadata": {
        "id": "dABZ12Bkugtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ],
      "metadata": {
        "id": "1w4wppuA4eNn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "MNozI65K4e7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa5d76e-a79a-4020-95da-971b96714498"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store While Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "hU998zkD4hpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "id": "uy9X-l_Iugtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d366e1-2792-4228-a8c5-0d4c2a00616f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ---\n",
            "**Define a RAG (Retrieval-Augmented Generation)**  **While Retrieval-Augmented\n",
            "Generation (RAG) offers substantial benefits, it also comes with its\n",
            "challenges.**  ---  ### What is Retrieval-Augmented Generation (RAG)?\n",
            "**Retrieval-Augmented Generation (RAG)** is a hybrid approach in Natural\n",
            "Language Processing (NLP) that combines the strengths of both retrieval-based\n",
            "and generation-based models.   - **Retrieval-based models**: These models fetch\n",
            "relevant information from a pre-existing database or corpus. They are efficient\n",
            "in providing accurate and contextually relevant responses based on the retrieved\n",
            "data.    - **Generation-based models**: These models generate text based on\n",
            "learned patterns from training data. They are capable of producing more flexible\n",
            "and creative responses but may sometimes lack factual accuracy.  By integrating\n",
            "these two approaches, RAG aims to leverage the factual accuracy of retrieval-\n",
            "based models and the flexibility of generation-based models to produce more\n",
            "accurate and contextually appropriate responses.  ### Benefits of RAG  1.\n",
            "**Improved Accuracy**: By retrieving relevant information from a large corpus,\n",
            "RAG can provide more accurate and factually correct responses.    2.\n",
            "**Contextual Relevance**: The retrieval component ensures that the generated\n",
            "text is contextually relevant to the query, enhancing the overall coherence and\n",
            "relevance of the response.    3. **Flexibility**: The generation component\n",
            "allows for more natural and varied responses, making the interaction feel more\n",
            "human-like.    4. **Scalability**: RAG can handle a wide range of topics and\n",
            "queries by leveraging a vast amount of pre-existing information.  ### Challenges\n",
            "of RAG  1. **Complexity**: Integrating retrieval and generation components can\n",
            "be technically complex, requiring sophisticated algorithms and significant\n",
            "computational resources.    2. **Latency**: The retrieval process can introduce\n",
            "latency, making the response time slower compared to purely generation-based\n",
            "models.    3. **Data Dependency**: The quality of the generated responses\n",
            "heavily depends on the quality and comprehensiveness of the retrieval corpus.\n",
            "Incomplete or biased data can lead to suboptimal responses.    4.\n",
            "**Maintenance**: Keeping the retrieval corpus up-to-date and relevant requires\n",
            "continuous effort and resources.    5. **Balancing Act**: Finding the right\n",
            "balance between retrieval and generation to ensure both accuracy and creativity\n",
            "can be challenging.  ### Conclusion  While Retrieval-Augmented Generation (RAG)\n",
            "offers substantial benefits by combining the strengths of retrieval-based and\n",
            "generation-based models, it also comes with its own set of challenges.\n",
            "Addressing these challenges is crucial for maximizing the effectiveness and\n",
            "efficiency of RAG systems in various NLP applications.  ---  I hope this\n",
            "detailed explanation helps! If you have any more questions or need further\n",
            "elaboration, feel free to ask.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Modular RAG Retriever"
      ],
      "metadata": {
        "id": "wWEvzcDHTX6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ],
      "metadata": {
        "id": "18wmqwJd4o62"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modular RAG Strategies"
      ],
      "metadata": {
        "id": "7qHm4saJ8cGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "_kvhIOdY8amp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea487b65-fce4-4bc3-9fe3-035c99747f62"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented Input"
      ],
      "metadata": {
        "id": "5TaQa7Dc7JwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+best_matching_record"
      ],
      "metadata": {
        "id": "X-hKjhIU7Jwg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "id": "ZhSO-fyZ7Jwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d354bdb6-4c4c-42be-b6fb-0e0cec8a2f8c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeWhile Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "qkyYx_MC7Jwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "id": "-V3srRHW7Jwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727f77d5-6bcc-4611-af9a-ad7b234e84a1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ---\n",
            "**Define a RAG (Retrieval-Augmented Generation)**  **While Retrieval-Augmented\n",
            "Generation (RAG) offers substantial benefits, it also comes with its\n",
            "challenges.**  ---  ### What is Retrieval-Augmented Generation (RAG)?\n",
            "Retrieval-Augmented Generation (RAG) is a hybrid approach in Natural Language\n",
            "Processing (NLP) that combines the strengths of retrieval-based methods and\n",
            "generative models. Here’s a detailed explanation:  1. **Retrieval-Based\n",
            "Methods**:    - These methods involve searching a large corpus of text to find\n",
            "relevant information or documents that can help answer a query.    - They are\n",
            "efficient and can provide accurate information if the relevant data exists in\n",
            "the corpus.    - Examples include search engines and question-answering systems\n",
            "that rely on pre-existing documents.  2. **Generative Models**:    - These\n",
            "models generate new text based on the input they receive.    - They are capable\n",
            "of producing coherent and contextually relevant responses, even if the exact\n",
            "answer is not present in the training data.    - Examples include models like\n",
            "GPT-3, which can generate human-like text based on prompts.  ### How Does RAG\n",
            "Work?  RAG combines these two approaches to leverage their respective strengths:\n",
            "1. **Retrieval Phase**:    - When a query is received, the system first\n",
            "retrieves relevant documents or passages from a large corpus.    - This\n",
            "retrieval is typically done using techniques like TF-IDF, BM25, or more advanced\n",
            "neural retrieval models.  2. **Generation Phase**:    - The retrieved documents\n",
            "are then fed into a generative model.    - The generative model uses the context\n",
            "provided by these documents to generate a more accurate and contextually\n",
            "relevant response.  ### Benefits of RAG  1. **Improved Accuracy**:    - By\n",
            "combining retrieval and generation, RAG can provide more accurate and\n",
            "contextually relevant answers.    - The retrieval step ensures that the\n",
            "generative model has access to relevant information, reducing the chances of\n",
            "generating incorrect or irrelevant responses.  2. **Efficiency**:    -\n",
            "Retrieval-based methods are generally faster and more efficient for finding\n",
            "specific information.    - By narrowing down the context before generation, RAG\n",
            "can produce high-quality responses more efficiently.  3. **Scalability**:    -\n",
            "RAG can scale to handle large corpora and diverse queries, making it suitable\n",
            "for a wide range of applications.  ### Challenges of RAG  1. **Complexity**:\n",
            "- Implementing a RAG system is more complex than using either retrieval-based\n",
            "methods or generative models alone.    - It requires careful integration of both\n",
            "components and fine-tuning to ensure optimal performance.  2. **Resource\n",
            "Intensive**:    - The retrieval phase requires maintaining and searching a large\n",
            "corpus, which can be resource-intensive.    - The generative phase requires\n",
            "significant computational power, especially for large models.  3. **Quality of\n",
            "Retrieved Documents**:    - The quality of the final response heavily depends on\n",
            "the relevance and quality of the retrieved documents.    - If the retrieval step\n",
            "fails to find relevant information, the generative model may produce less\n",
            "accurate responses.  4. **Latency**:    - Combining retrieval and generation can\n",
            "introduce additional latency, which may be a concern for real-time applications.\n",
            "### Conclusion  Retrieval-Augmented Generation (RAG) is a powerful approach in\n",
            "NLP that combines the strengths of retrieval-based methods and generative models\n",
            "to provide accurate and contextually relevant responses. While it offers\n",
            "substantial benefits in terms of accuracy, efficiency, and scalability, it also\n",
            "comes with challenges such as complexity, resource intensity, and dependency on\n",
            "the quality of retrieved documents. Addressing these challenges is crucial for\n",
            "the successful implementation of RAG systems.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}