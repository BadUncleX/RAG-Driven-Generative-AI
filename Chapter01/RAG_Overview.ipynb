{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMO5KJUbAkcKSWT3SzM9cwK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introducing Naïve, Advanced, and Modular RAG\n",
        "\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "This notebook introduces Naïve, Advanced, and Modular RAG through basic educational examples.\n",
        "\n",
        "It explores keyword matching, vector search, and index-based retrieval methods. Using OpenAI's GPT models, it generates responses based on input queries and retrieved documents.\n",
        "\n",
        "The modular RAG system offers flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "- Introduction of Naïve, Advanced, and Modular RAG\n",
        "- Environment setup for OpenAI API integration\n",
        "- Generator function using GPT models\n",
        "- Formatted response printing\n",
        "- **Data** setup with a list of documents (db_records)\n",
        "- **Query** user request\n",
        "- **1.Naïve RAG**:\n",
        "  - Keyword search and matching function\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT\n",
        "- **2.Advanced RAG**:\n",
        "  - Vector search:\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "  - Index-based retrieval:\n",
        "    - Setup of TF-IDF vectorizer and matrix\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "- **3.Modular RAG Retriever**:\n",
        "  - RetrievalComponent class with methods for keyword, vector, and indexed search\n",
        "  - Usage example with different retrieval methods\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT"
      ],
      "metadata": {
        "id": "sqpsYn49QWSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Environment"
      ],
      "metadata": {
        "id": "o01-IM8bTc5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.19.0"
      ],
      "metadata": {
        "id": "8VCfbN0YwHbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1ac780-22cb-4e3f-84ce-d051c1989623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.19.0\n",
            "  Downloading openai-1.19.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.19.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.19.0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.19.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.19.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#API Key\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965739a4-b13f-4b46-cf27-3cf6ba9b98a1",
        "id": "myXJn33zbqTR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "Oefvqp21Ba07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Generator\n"
      ],
      "metadata": {
        "id": "WuZ7jr4Rs36U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4-o\" # or select gpt-3.5-turbo\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "def call_gpt4_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ],
      "metadata": {
        "id": "qwCNTW9fs36U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formatted response"
      ],
      "metadata": {
        "id": "bVKe9VF0HIHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ],
      "metadata": {
        "id": "oG8I2Kb2HFhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # The Data"
      ],
      "metadata": {
        "id": "Qv1ExPiZdJRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\"\n",
        "]"
      ],
      "metadata": {
        "id": "45CFxG4Fgcju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIQ7NK92g7EC",
        "outputId": "b24b751e-5b6b-4085-e66b-98e6a33eb346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Query"
      ],
      "metadata": {
        "id": "aL7cHuuLhQ5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"define a rag store\""
      ],
      "metadata": {
        "id": "qARk6gtohSXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation without augmentation"
      ],
      "metadata": {
        "id": "U4lLAvV6NIn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(query)\n",
        "# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a28b837-fb7a-4116-f09f-6cade702527d",
        "id": "iO-k1Tq4MqmP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "It seems like you're asking for an elaboration on a fragmented input that\n",
            "appears to spell out \"define a rag store.\" If that's correct, here's a detailed\n",
            "explanation:  ### Define a Rag Store  **Definition:** A \"rag store\" typically\n",
            "refers to a store or a place where old clothes, rags, or fabric remnants are\n",
            "sold. These items can be used for various purposes such as cleaning, crafting,\n",
            "or even recycled into new products. Rag stores might also deal in second-hand\n",
            "clothing, vintage fabrics, and other textile-related items.  **Purpose and\n",
            "Uses:** 1. **Cleaning Supplies:** Rags are commonly used in both industrial and\n",
            "domestic settings for cleaning because they are absorbent and reusable. 2.\n",
            "**Crafting Materials:** Artists and crafters often use old fabrics and rags\n",
            "sourced from rag stores for projects like quilting, patchwork, or creative\n",
            "fashion design. 3. **Recycling and Upcycling:** Environmentally conscious\n",
            "individuals and businesses buy old fabrics to recycle them into new textile\n",
            "products, reducing waste and the demand for new raw materials. 4. **Cost-\n",
            "Effective Shopping:** For individuals looking for inexpensive clothing options,\n",
            "rag stores can provide affordable alternatives to buying new clothes.\n",
            "**Historical Context:** Historically, rag stores played a crucial role in the\n",
            "textile industry. Before the advent of modern recycling methods and the mass\n",
            "production of textiles, rag stores were essential for sourcing materials for\n",
            "paper production and affordable clothing. They were also a part of the economy\n",
            "that dealt with waste management by repurposing used textiles.  **Modern\n",
            "Relevance:** In contemporary times, with increasing awareness about\n",
            "environmental issues and sustainable living, rag stores have seen a resurgence.\n",
            "They align well with the principles of reducing waste, reusing materials, and\n",
            "supporting local economies.  If this interpretation matches your query, this\n",
            "should provide a comprehensive understanding of what a rag store is and its\n",
            "significance. If you meant something else, please provide additional details or\n",
            "clarify your request.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Naïve Retrieval Augmented Generation(RAG)"
      ],
      "metadata": {
        "id": "JFqh9rr81SUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword search and matching"
      ],
      "metadata": {
        "id": "Wu8vteKmS_qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY1JU0Ush_l4",
        "outputId": "a759e5e4-210b-4055-95e8-d00b5da0a4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 1\n",
            "Response:\n",
            "---------------\n",
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmented input"
      ],
      "metadata": {
        "id": "r2zKQhiO0Fcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+best_matching_record"
      ],
      "metadata": {
        "id": "r_7ymSxG0Fcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NTfxzum0PT2",
        "outputId": "efda25bb-1566-44e2-e2b3-1de0c8581700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeRetrieval Augmented Generation (RAG) represents a\n",
            "sophisticated hybrid approach in the field of artificial intelligence,\n",
            "particularly within the realm of natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "8Ui8wH4k3_g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe9eaac-1cb4-49a4-8a96-ca97d5d86185",
        "id": "Jh8BsnUy0Fcs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "The term \"Retrieval Augmented Generation (RAG)\" refers to an advanced hybrid\n",
            "approach in the field of artificial intelligence, particularly within the realm\n",
            "of natural language processing (NLP). This method combines the strengths of two\n",
            "major components in AI: retrieval systems and generative models.  **Retrieval\n",
            "Systems**: These are designed to fetch relevant information from a large dataset\n",
            "or database. In the context of NLP, retrieval systems are used to find text\n",
            "segments that are relevant to a given query. This is crucial for tasks where the\n",
            "answer needs to be supported by specific data, such as in question answering\n",
            "systems.  **Generative Models**: These models are capable of generating coherent\n",
            "text based on the input they receive. In NLP, generative models are often used\n",
            "for tasks like text completion, summarization, and translation. They are trained\n",
            "on large corpora of text and learn to predict the probability of a sequence of\n",
            "words.  **Hybrid Approach**: RAG leverages the strengths of both retrieval and\n",
            "generation. First, when a query is received, the retrieval component searches\n",
            "through a dataset to find relevant information. This information is then passed\n",
            "to the generative model, which uses it to produce a coherent and contextually\n",
            "appropriate response. This approach ensures that the generated responses are not\n",
            "only fluent but also factually accurate and informative, as they are based on\n",
            "retrieved data that is relevant to the query.  This hybrid method is\n",
            "particularly useful in scenarios where the accuracy of the information is\n",
            "critical, such as in educational tools, customer support, and any application\n",
            "requiring reliable knowledge dissemination. By combining retrieval and\n",
            "generation, RAG provides a powerful tool for enhancing the capabilities of AI\n",
            "systems in understanding and generating human language.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Advanced Retrieval Augmented Generation(RAG)"
      ],
      "metadata": {
        "id": "zJH2__0iTUr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.Vector search"
      ],
      "metadata": {
        "id": "awyjcn35jFiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record\n",
        "\n",
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCBbY4qLc8qh",
        "outputId": "08a75d66-d50d-44d6-8fe6-520115bc46b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.087\n",
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented input"
      ],
      "metadata": {
        "id": "51fZC6Oe2G9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ],
      "metadata": {
        "id": "4dcnK7OGx5e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3uk-91x049J",
        "outputId": "1f71d425-0339-480d-dd73-476f2b8732ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store While Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "wFDF6hbi2LF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJC-mA5ftxFU",
        "outputId": "32bd725c-6c79-4d62-de7a-6963b06f8db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "The content you've provided seems to be about \"Retrieval Augmented Generation\n",
            "(RAG)\" which is a technique used in the field of Natural Language Processing\n",
            "(NLP). Let's define RAG and discuss its benefits and challenges in a more\n",
            "coherent form.  **Retrieval Augmented Generation (RAG)**: RAG is an NLP\n",
            "technique that combines the power of language models with information retrieval\n",
            "methods to enhance the generation of text. This approach typically involves\n",
            "retrieving relevant documents or data from a large corpus and then using this\n",
            "information to assist a generative model in producing more accurate and\n",
            "contextually relevant responses.  **Benefits of RAG**: 1. **Enhanced Accuracy\n",
            "and Relevance**: By retrieving information from relevant documents, RAG can\n",
            "generate responses that are not only contextually appropriate but also factually\n",
            "accurate. 2. **Richer Content**: The integration of retrieved data allows the\n",
            "model to produce richer and more detailed content, which can be particularly\n",
            "useful in applications like content creation, question answering, and more. 3.\n",
            "**Scalability**: RAG can leverage existing databases and corpora, making it\n",
            "scalable and adaptable to various domains without requiring the generative model\n",
            "to learn and store all the information internally.  **Challenges of RAG**: 1.\n",
            "**Complexity in Integration**: Combining retrieval systems with generative\n",
            "models can be technically challenging, requiring sophisticated mechanisms to\n",
            "ensure smooth operation and effective integration of the two components. 2.\n",
            "**Latency Issues**: The retrieval process can add latency to the response\n",
            "generation, which might be a critical issue in real-time applications. 3.\n",
            "**Quality of Source Data**: The effectiveness of RAG heavily depends on the\n",
            "quality and relevance of the data it retrieves. Poor data quality can lead to\n",
            "inaccurate or misleading outputs. 4. **Handling Ambiguity**: The model must\n",
            "effectively handle cases where multiple documents or data points are retrieved,\n",
            "some of which might contain conflicting information.  In summary, while\n",
            "Retrieval Augmented Generation offers substantial benefits by enhancing the\n",
            "quality and relevance of generated text, it also presents several challenges\n",
            "that need to be addressed to fully leverage its capabilities. These include\n",
            "technical complexities, potential increases in response time, dependency on the\n",
            "quality of retrieved data, and the need to manage ambiguities effectively.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.Index-based search"
      ],
      "metadata": {
        "id": "t7djpPBpm0M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRarT_fym2XC",
        "outputId": "7ae1c449-0bdb-4247-fd2e-fb334f091572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.216\n",
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(tfidf_df)\n",
        "\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbokQ2eacHjM",
        "outputId": "2a8894e5-f62a-42da-9830-91ea6226ad41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ability    access  accuracy  accurate    adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "1   0.000000  0.000000   0.00000  0.216814  0.00000    0.000000     0.000000   \n",
            "2   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "3   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "4   0.000000  0.000000   0.00000  0.000000  0.00000    0.236798     0.000000   \n",
            "5   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "6   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "7   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "8   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "9   0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "10  0.186722  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "11  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "12  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "13  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "14  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "15  0.000000  0.172817   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "16  0.000000  0.318332   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "17  0.000000  0.000000   0.00000  0.207376  0.00000    0.000000     0.000000   \n",
            "18  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "19  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "20  0.000000  0.000000   0.00000  0.000000  0.27489    0.000000     0.000000   \n",
            "21  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "22  0.000000  0.174434   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "23  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "24  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.000000   \n",
            "25  0.000000  0.000000   0.22915  0.000000  0.00000    0.000000     0.000000   \n",
            "26  0.000000  0.000000   0.00000  0.000000  0.00000    0.000000     0.173873   \n",
            "\n",
            "      advice  algorithms    allows  ...      vast   vector      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "7   0.000000    0.221347  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.294228   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.000000  0.22394  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "19  0.244567    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291828  ...  0.000000  0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000  0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.173873  0.00000  0.000000   \n",
            "\n",
            "       where     which     while      wide      with    within   without  \n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.260831  0.000000  \n",
            "1   0.000000  0.000000  0.000000  0.000000  0.160002  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.245200  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179186  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.181285  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "15  0.189693  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.000000  0.000000  0.257860  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "19  0.217317  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.191365  0.000000  0.291828  \n",
            "22  0.000000  0.000000  0.000000  0.215477  0.141298  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.329094  0.000000  0.215802  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.133742  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[27 rows x 292 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented input"
      ],
      "metadata": {
        "id": "dABZ12Bkugtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\" \"+best_matching_record"
      ],
      "metadata": {
        "id": "1w4wppuA4eNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNozI65K4e7u",
        "outputId": "81196170-b099-4303-f852-36ac6f66c33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store While Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "hU998zkD4hpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d61d996d-3730-4534-b419-5ffc29362020",
        "id": "uy9X-l_Iugtt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "The content you've provided seems to be about \"Retrieval Augmented Generation\n",
            "(RAG)\" which is a technique used in the field of Natural Language Processing\n",
            "(NLP). Let's define RAG and discuss its benefits and challenges in more detail.\n",
            "### Definition of Retrieval Augmented Generation (RAG)  Retrieval Augmented\n",
            "Generation (RAG) is a hybrid approach that combines the power of retrieval-based\n",
            "and generative NLP models. In this approach, a retrieval system is first used to\n",
            "fetch relevant documents or information from a large corpus or database. This\n",
            "retrieved information is then fed into a generative model, which uses it to\n",
            "generate responses or complete tasks. This method leverages both the precision\n",
            "of retrieval systems in finding relevant information and the flexibility of\n",
            "generative models in producing coherent and contextually appropriate text.  ###\n",
            "Benefits of RAG  1. **Improved Accuracy**: By using relevant context from\n",
            "retrieved documents, RAG can generate more accurate and contextually appropriate\n",
            "responses. 2. **Enhanced Knowledge**: RAG can pull in external knowledge that\n",
            "may not be present in the training data of the generative model, leading to\n",
            "responses that are more informed and detailed. 3. **Scalability**: Since RAG can\n",
            "dynamically pull information from a large corpus, it doesn't need to store all\n",
            "the information within the model, making it more scalable and adaptable to new\n",
            "information. 4. **Flexibility**: RAG can be applied to various tasks in NLP,\n",
            "including question answering, chatbots, and content generation, making it a\n",
            "versatile tool.  ### Challenges of RAG  1. **Complexity**: The integration of\n",
            "retrieval systems with generative models adds complexity to the model\n",
            "architecture and the training process. 2. **Latency**: Retrieving documents and\n",
            "generating responses can add latency, which might be a concern for applications\n",
            "requiring real-time responses. 3. **Data Dependence**: The performance of RAG\n",
            "heavily depends on the quality and relevance of the data in the retrieval\n",
            "corpus. Poor quality or irrelevant data can lead to inaccurate or misleading\n",
            "outputs. 4. **Resource Intensity**: Running retrieval systems alongside\n",
            "generative models can be resource-intensive, requiring significant computational\n",
            "power and memory.  In summary, while Retrieval Augmented Generation offers\n",
            "substantial benefits by enhancing the accuracy and richness of generative models\n",
            "with the precision of retrieval systems, it also presents challenges such as\n",
            "increased complexity, potential latency issues, dependence on the quality of\n",
            "retrieved data, and high resource requirements. These factors must be carefully\n",
            "managed to effectively utilize RAG in practical applications.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Modular RAG Retriever"
      ],
      "metadata": {
        "id": "wWEvzcDHTX6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ],
      "metadata": {
        "id": "18wmqwJd4o62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modular RAG Strategies"
      ],
      "metadata": {
        "id": "7qHm4saJ8cGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kvhIOdY8amp",
        "outputId": "58d8fc5d-9455-4ff9-fbc4-5f9e6757ed46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "While Retrieval Augmented Generation (RAG) offers substantial benefits, it also\n",
            "comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented Input"
      ],
      "metadata": {
        "id": "5TaQa7Dc7JwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+best_matching_record"
      ],
      "metadata": {
        "id": "X-hKjhIU7Jwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f669e38-61d0-49ab-bfc1-a8ea2f010c14",
        "id": "ZhSO-fyZ7Jwg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag storeWhile Retrieval Augmented Generation (RAG) offers substantial\n",
            "benefits, it also comes with its challenges.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "qkyYx_MC7Jwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4ff343-95e7-452e-8a19-c9a668b4dc19",
        "id": "-V3srRHW7Jwh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "The content you've provided seems to be about \"Retrieval Augmented Generation\n",
            "(RAG)\" and mentions its benefits and challenges. Let's elaborate on this\n",
            "concept:  **Retrieval Augmented Generation (RAG)** is a technique used in\n",
            "natural language processing that combines the capabilities of pre-trained\n",
            "language models with information retrieval methods to enhance the generation of\n",
            "text. This approach allows the model to dynamically retrieve external knowledge\n",
            "from a large corpus of documents and use this information to generate more\n",
            "accurate, informative, and contextually relevant responses.  ### Benefits of\n",
            "RAG: 1. **Enhanced Knowledge**: RAG models can access a vast amount of\n",
            "information beyond what they were originally trained on. This allows them to\n",
            "provide responses that are not only contextually relevant but also factually\n",
            "accurate, drawing from up-to-date and expansive databases. 2. **Improved\n",
            "Contextual Relevance**: By retrieving information related to the query context,\n",
            "RAG models can generate responses that are more aligned with the specific needs\n",
            "and nuances of the input. 3. **Flexibility and Adaptability**: These models can\n",
            "adapt to new topics and content areas without needing retraining, as they can\n",
            "pull the latest information from external sources. 4. **Reduction in Training\n",
            "Costs**: Since RAG models leverage external databases, they can be effective\n",
            "with potentially smaller and less costly training datasets, relying instead on\n",
            "the richness of the external texts.  ### Challenges of RAG: 1. **Complexity in\n",
            "Integration**: Integrating retrieval systems with generative models can be\n",
            "technically challenging. The system needs to efficiently query and retrieve\n",
            "relevant documents in real-time, which requires sophisticated engineering\n",
            "solutions. 2. **Quality of Sources**: The effectiveness of a RAG model heavily\n",
            "depends on the quality and reliability of the external sources it accesses. Poor\n",
            "quality or biased information can lead to inaccurate or misleading outputs. 3.\n",
            "**Latency Issues**: Retrieval operations can introduce latency, making the\n",
            "response generation slower, which might be critical for real-time applications.\n",
            "4. **Handling Ambiguity**: Determining the relevance of retrieved documents and\n",
            "integrating that information into a coherent response can be challenging,\n",
            "especially when the input query is ambiguous or when multiple interpretations\n",
            "are possible. 5. **Cost of Retrieval**: Depending on the implementation, the\n",
            "computational and financial costs associated with accessing large external\n",
            "databases can be significant.  In summary, while Retrieval Augmented Generation\n",
            "offers substantial benefits by enhancing the knowledge and relevance of\n",
            "generated responses, it also presents several technical and practical challenges\n",
            "that need to be addressed to maximize its effectiveness.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}